import streamlit as st
import google.generativeai as genai
import time

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ê²°ì •ì¥ì•  ê³ ë¯¼ í•´ê²°ì‚¬",
    page_icon="ğŸ¤”",
    layout="centered"
)

# API í‚¤ ì„¤ì •
genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if "initial_prompt_sent" not in st.session_state:
    st.session_state.initial_prompt_sent = False

if "question_count" not in st.session_state:
    st.session_state.question_count = 0

if "chat_session" not in st.session_state:
    # Gemini ëª¨ë¸ ì´ˆê¸°í™” ë° ì±„íŒ… ì„¸ì…˜ ì‹œì‘
    model = genai.GenerativeModel('gemini-1.5-flash')
    st.session_state.chat_session = model.start_chat(history=[])

# í—¤ë” ë””ìì¸
st.title("ğŸ¤” ê²°ì •ì¥ì•  ê³ ë¯¼ í•´ê²°ì‚¬")

# ì´ˆê¸° ë©”ì‹œì§€ ë°œì†¡ (í•œ ë²ˆë§Œ)
if not st.session_state.initial_prompt_sent:
    initial_message = """
    ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ì—¬ëŸ¬ë¶„ì˜ ê³ ë¯¼ì„ í•¨ê»˜ í•´ê²°í•´ë‚˜ê°€ëŠ” AI ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.
    ì–´ë–¤ ê²°ì •ì„ ë‚´ë¦¬ê¸° ì–´ë ¤ìš´ ìƒí™©ì´ì‹ ê°€ìš”? ì²œì²œíˆ ì´ì•¼ê¸°í•´ì£¼ì„¸ìš”.
    """
    st.session_state.chat_history.append({
        "role": "model",
        "parts": [initial_message]
    })
    st.session_state.initial_prompt_sent = True

# ëŒ€í™” ê¸°ë¡ í‘œì‹œ
for message in st.session_state.chat_history:
    with st.chat_message("assistant" if message["role"] == "model" else "user"):
        st.markdown(message["parts"][0])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ê³ ë¯¼ì´ ìˆë‹¤ë©´ ììœ ë¡­ê²Œ ì´ì•¼ê¸°í•´ì£¼ì„¸ìš”..."):
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ ë° ì €ì¥
    st.chat_message("user").markdown(prompt)
    st.session_state.chat_history.append({
        "role": "user",
        "parts": [prompt]
    })

    try:
        # AI ì‘ë‹µ ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
        full_prompt = f"""
        ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ê²°ì •ì¥ì•  ê³ ë¯¼ì„ ë“£ê³ , ê³µê°í•˜ë©°, ê·¸ ì´ë©´ì— ìˆ¨ê²¨ì§„ ì§„ì§œ ë§ˆìŒì„ ì´í•´í•˜ë„ë¡ ë•ëŠ” ì¹œì ˆí•œ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.

        í˜„ì¬ ì§ˆë¬¸ íšŸìˆ˜: {st.session_state.question_count}

        ì—„ê²©í•œ ì§ˆë¬¸ ê·œì¹™:
        1. ì§ˆë¬¸ì€ ì ˆëŒ€ë¡œ 2íšŒë¥¼ ì´ˆê³¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
        2. í˜„ì¬ ì§ˆë¬¸ íšŸìˆ˜ê°€ 2íšŒ ì´ìƒì´ë©´, ë” ì´ìƒì˜ ì§ˆë¬¸ ì—†ì´ ë°˜ë“œì‹œ ìµœì¢… ê²°ë¡ ì„ ë„ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.
        3. ê° ì‘ë‹µì—ëŠ” ë°˜ë“œì‹œ í•˜ë‚˜ì˜ ì§ˆë¬¸ë§Œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

        ì§ˆë¬¸ ë‹¨ê³„ë³„ ì§€ì¹¨:
        - ì²« ë²ˆì§¸ ì§ˆë¬¸ (í˜„ì¬ ì¹´ìš´íŠ¸ê°€ 0ì¼ ë•Œ):
          * í˜„ì¬ ìƒí™©ì„ ë” ìì„¸íˆ ì´í•´í•˜ê¸° ìœ„í•œ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•˜ë‚˜ë§Œ í•˜ì„¸ìš”.
          * ì˜ˆ: "êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ì„ íƒì§€ë“¤ ì‚¬ì´ì—ì„œ ê³ ë¯¼ì´ ë˜ì‹œë‚˜ìš”?"

        - ë‘ ë²ˆì§¸ ì§ˆë¬¸ (í˜„ì¬ ì¹´ìš´íŠ¸ê°€ 1ì¼ ë•Œ):
          * ê³ ë¯¼ì˜ ê·¼ë³¸ ì›ì¸ì„ íŒŒì•…í•˜ê¸° ìœ„í•œ ì‹¬ì¸µì ì¸ ì§ˆë¬¸ì„ í•˜ë‚˜ë§Œ í•˜ì„¸ìš”.
          * ì˜ˆ: "ê·¸ ì„ íƒì§€ë“¤ ì¤‘ì—ì„œ íŠ¹ë³„íˆ ë” ëŒë¦¬ëŠ” ìª½ì´ ìˆë‚˜ìš”?"

        - ê²°ë¡  ë„ì¶œ (í˜„ì¬ ì¹´ìš´íŠ¸ê°€ 2 ì´ìƒì¼ ë•Œ):
          ë°˜ë“œì‹œ ë‹¤ìŒ êµ¬ì¡°ë¡œ ìµœì¢… í•´ê²°ì±…ì„ ì œì‹œí•˜ì„¸ìš”:

          **í˜„ì¬ ìƒí™© ë¶„ì„**:
          (ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™”ë¥¼ í†µí•´ íŒŒì•…í•œ ì‚¬ìš©ìì˜ ìƒí™© ìš”ì•½)

          **ê³ ë¯¼ì˜ í•µì‹¬**:
          (ì§ˆë¬¸ë“¤ì„ í†µí•´ íŒŒì•…í•œ ì§„ì§œ ê³ ë¯¼ì˜ ì›ì¸)

          **ì¶”ì²œ ì¡°ì¹˜**:
          (êµ¬ì²´ì ì´ê³  ì‹¤ì²œ ê°€ëŠ¥í•œ í•´ê²° ë°©ì•ˆ)

        ê³µê°ê³¼ ì†Œí†µ:
        - ê° ì‘ë‹µì€ ë°˜ë“œì‹œ ê³µê° í‘œí˜„ìœ¼ë¡œ ì‹œì‘í•˜ì„¸ìš”.
        - ë”°ëœ»í•˜ê³  ì¹œê·¼í•œ ì–´ì¡°ë¥¼ ìœ ì§€í•˜ì„¸ìš”.
        - ì‚¬ìš©ìì˜ ê°ì •ì— ì¶©ë¶„íˆ ê³µê°í•˜ì„¸ìš”.

        í•´ê²°ì±… ì œì‹œ ë°©ì‹:
        - ì‚¬ìš©ìê°€ ìŠ¤ìŠ¤ë¡œ ê²°ì •í•  ìˆ˜ ìˆë„ë¡ ë•ëŠ” ì¡°ì–¸ì„ ì œê³µí•˜ì„¸ìš”.
        - ê°•ì••ì ì¸ ì–´íˆ¬ëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
        - ì‹¤ì²œ ê°€ëŠ¥í•œ êµ¬ì²´ì  ë°©ì•ˆì„ ì œì‹œí•˜ì„¸ìš”.

        ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ ê³ ë¯¼: {prompt}
        """

        # AI ì‘ë‹µ ìƒì„± ë° ì‹¤ì‹œê°„ í‘œì‹œ
        response = st.session_state.chat_session.send_message(
            full_prompt,
            stream=True
        )

        # ì‘ë‹µ í‘œì‹œë¥¼ ìœ„í•œ ì»¨í…Œì´ë„ˆ ìƒì„±
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            
            # ì‘ë‹µì„ ì‹¤ì‹œê°„ìœ¼ë¡œ í‘œì‹œ
            for chunk in response:
                if chunk.text:
                    full_response += chunk.text
                    # ì„ì‹œ ì»¤ì„œë¥¼ ì¶”ê°€í•˜ì—¬ íƒ€ì´í•‘ íš¨ê³¼ ìƒì„±
                    message_placeholder.markdown(full_response + "â–Œ")
                    time.sleep(0.01)
            
            # ìµœì¢… ì‘ë‹µ í‘œì‹œ (ì»¤ì„œ ì œê±°)
            message_placeholder.markdown(full_response)
            
            # ëŒ€í™” ê¸°ë¡ì— ì €ì¥
            st.session_state.chat_history.append({
                "role": "model",
                "parts": [full_response]
            })

            # ì§ˆë¬¸ì´ í¬í•¨ëœ ê²½ìš°ì—ë§Œ ì¹´ìš´í„° ì¦ê°€
            if "?" in full_response and st.session_state.question_count < 2:
                st.session_state.question_count += 1

    except Exception as e:
        error_message = f"ì±—ë´‡ ì‘ë‹µ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}. API í‚¤ë¥¼ í™•ì¸í•˜ê±°ë‚˜ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        st.error(error_message)

# í‘¸í„° ì¶”ê°€
st.markdown("---")
st.markdown("ğŸ’¡ ëª¨ë“  ëŒ€í™”ëŠ” ë¹„ê³µê°œë¡œ ì§„í–‰ë˜ë©°, ì•ˆì „í•˜ê²Œ ë³´í˜¸ë©ë‹ˆë‹¤.")
st.markdown("<br><br><br>", unsafe_allow_html=True)
